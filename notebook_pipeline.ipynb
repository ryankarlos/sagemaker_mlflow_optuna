{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# FM Optuna Pipeline\n",
    "\n",
    "Run Factorization Machines hyperparameter optimization with Optuna and MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d228e465-f67a-4369-bbf0-835e4d364672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.notebook_job_step import NotebookJobStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "import optuna\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "import os\n",
    "import dask\n",
    "from typing import List, Dict\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_steps_for_pipeline(\n",
    "    config_dict: dict,\n",
    "    image_uri: str,\n",
    "    notebook_artifacts: str,\n",
    "    input_notebook_name: str,\n",
    "    kernel_name: str = \"python3\",\n",
    "    instance_type: str = \"ml.m5.xlarge\",\n",
    "    role: str = None,\n",
    "    **params,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Define notebook job steps for the pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_dict : dict\n",
    "        Dictionary mapping config names to parameter lists\n",
    "        e.g., {\"config_1\": {\"n_users\": 5000, \"n_games\": 100}, ...}\n",
    "    image_uri : str\n",
    "        SageMaker container image URI\n",
    "    notebook_artifacts : str\n",
    "        S3 path for notebook artifacts\n",
    "    input_notebook_name : str\n",
    "        Name of the notebook to execute\n",
    "    kernel_name : str\n",
    "        Jupyter kernel name\n",
    "    instance_type : str\n",
    "        SageMaker instance type\n",
    "    role : str\n",
    "        SageMaker execution role ARN\n",
    "    **params : dict\n",
    "        Additional parameters passed to all notebook jobs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of NotebookJobStep objects\n",
    "    \"\"\"\n",
    "    pipeline_steps = []\n",
    "    if role is None:\n",
    "        role = sagemaker.get_execution_role()\n",
    "\n",
    "    for config_name, config_params in config_dict.items():\n",
    "        # Merge config params with global params\n",
    "        nb_job_params = {\n",
    "            \"config_name\": config_name,\n",
    "            **config_params,\n",
    "            **params,\n",
    "        }\n",
    "\n",
    "        train_description = f\"FM Optuna training for {config_name}\"\n",
    "        train_id = f\"fm-train-{config_name}\"\n",
    "\n",
    "        nb_step = NotebookJobStep(\n",
    "            name=train_id,\n",
    "            description=train_description,\n",
    "            notebook_job_name=train_id,\n",
    "            image_uri=image_uri,\n",
    "            kernel_name=kernel_name,\n",
    "            display_name=train_id,\n",
    "            role=role,\n",
    "            s3_root_uri=notebook_artifacts,\n",
    "            input_notebook=input_notebook_name,\n",
    "            instance_type=instance_type,\n",
    "            parameters=nb_job_params,\n",
    "            max_runtime_in_seconds=86400,  # 24 hours\n",
    "            max_retry_attempts=3,\n",
    "        )\n",
    "        pipeline_steps.append(nb_step)\n",
    "\n",
    "    return pipeline_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "pipeline_name = \"fm-optuna-pipeline\"\n",
    "train_notebook = \"fm_train.ipynb\"\n",
    "bucket = \"fm-gambling-recommender-dev-376337229415\"\n",
    "subfolder_name = \"fm-training\"\n",
    "region = \"us-east-1\"\n",
    "image_uri = f\"arn:aws:sagemaker:{region}:885854791233:image/sagemaker-distribution-cpu\"\n",
    "kernel_name = \"python3\"\n",
    "notebook_artifacts = f\"s3://{bucket}/{subfolder_name}\"\n",
    "\n",
    "# Training parameters\n",
    "max_trials = \"20\"\n",
    "early_stopping = \"5\"\n",
    "experiment_name = \"fm_gambling_optuna_2025\"\n",
    "\n",
    "\n",
    "# mlflow server details\n",
    "\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"arn:aws:sagemaker:us-east-1:376337229415:mlflow-app/app-JZITH5VWKAWZ\"\n",
    "\n",
    "# Different dataset configurations to test\n",
    "config_dict = {\n",
    "    \"small\": {\n",
    "        \"n_users\": \"1000\",\n",
    "        \"n_games\": \"50\",\n",
    "        \"n_days\": \"90\",\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"n_users\": \"5000\",\n",
    "        \"n_games\": \"100\",\n",
    "        \"n_days\": \"180\",\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"n_users\": \"10000\",\n",
    "        \"n_games\": \"200\",\n",
    "        \"n_days\": \"365\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Define Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 pipeline steps\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_trials\": max_trials,\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"early_stopping\": early_stopping,\n",
    "}\n",
    "\n",
    "pipeline_steps = define_steps_for_pipeline(\n",
    "    config_dict,\n",
    "    image_uri,\n",
    "    notebook_artifacts,\n",
    "    train_notebook,\n",
    "    kernel_name,\n",
    "    instance_type,\n",
    "    **params\n",
    ")\n",
    "\n",
    "print(f\"Created {len(pipeline_steps)} pipeline steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Local Mode Execution (Testing)\n",
    "\n",
    "Use local mode to test the pipeline before running on SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.local.entities:Starting execution for pipeline fm-optuna-pipeline. Execution ID is 74b37890-6f17-4dd7-bcd6-aa53f1e5ca70\n",
      "INFO:sagemaker.local.entities:Starting pipeline step: 'fm-train-large'\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.local.entities:Pipeline step 'fm-train-large' FAILED. Failure message is: ImportError: Docker Compose is not installed. Local Mode features will not work without docker compose. For more information on how to install 'docker compose', please, see https://docs.docker.com/compose/install/\n",
      "INFO:sagemaker.local.entities:Pipeline execution 74b37890-6f17-4dd7-bcd6-aa53f1e5ca70 FAILED because step 'fm-train-large' failed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "session = LocalPipelineSession()\n",
    "pipeline = Pipeline(name=pipeline_name, steps=pipeline_steps, sagemaker_session=session)\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline.create(role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## SageMaker Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute on SageMaker\n",
    "\n",
    "session = PipelineSession()\n",
    "pipeline = Pipeline(name=pipeline_name, steps=pipeline_steps, sagemaker_session=session)\n",
    "\n",
    "if role is None:\n",
    "    role = sagemaker.get_execution_role()\n",
    "\n",
    "pipeline.upsert(role)\n",
    "execution = pipeline.start()\n",
    "print(execution.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Aggregate Results\n",
    "\n",
    "After pipeline completes, aggregate Optuna studies from all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09012676-d8bb-4718-8b49-efd8545dd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def copy_optuna_study_to_db(source_db_file, target_db_file):\n",
    "    study_name = optuna.study.get_all_study_names(\n",
    "        storage=f\"sqlite:///{source_db_file}\"\n",
    "    )[0]\n",
    "    optuna.copy_study(\n",
    "        from_study_name=study_name,\n",
    "        from_storage=f\"sqlite:///{source_db_file}\",\n",
    "        to_storage=f\"sqlite:///{target_db_file}\",\n",
    "    )\n",
    "\n",
    "\n",
    "def copy_all_studies_to_new_file(target_db_file, source_db_dir):\n",
    "    if Path(target_db_file).exists():\n",
    "        print(f\"Removing existing optuna target db {target_db_file}\")\n",
    "        Path(target_db_file).unlink()\n",
    "    for source_file in Path(source_db_dir).iterdir():\n",
    "        if source_file.suffix == \".db\":\n",
    "            copy_optuna_study_to_db(source_file, target_db_file)\n",
    "\n",
    "\n",
    "def download_single_artifact(run_info: Dict, folder, base_path: str) -> str:\n",
    "    \"\"\"Download an mlflow artifact for a single run based on root folder\"\"\"\n",
    "    try:\n",
    "        run_id = run_info[\"run_id\"]\n",
    "        run_path = Path(base_path)\n",
    "        run_path.mkdir(exist_ok=True)\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        artifacts = client.list_artifacts(run_id)\n",
    "        for artifact in artifacts:\n",
    "            if folder in artifact.path:\n",
    "                client.download_artifacts(run_id, artifact.path, str(run_path))\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading artifacts for run {run_id}: {str(e)}\"\n",
    "\n",
    "\n",
    "def get_experiment_runs(experiment_name) -> List[Dict]:\n",
    "    \"\"\"Get all runs for the specified experiment\"\"\"\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        raise ValueError(f\"Experiment {experiment_name} not found\")\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    return runs.to_dict(\"records\")\n",
    "\n",
    "\n",
    "def download_artifacts_across_runs(experiment_name: str, folder: str, local_path: str):\n",
    "    runs = get_experiment_runs(experiment_name)\n",
    "    local_path = Path(local_path)\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "    # Create delayed objects for each download task\n",
    "    delayed_tasks = [\n",
    "        dask.delayed(download_single_artifact)(run, folder, str(local_path))\n",
    "        for run in runs\n",
    "    ]\n",
    "    # Compute all tasks\n",
    "    dask.compute(*delayed_tasks)\n",
    "\n",
    "\n",
    "def execute_study_agg_pipeline(\n",
    "    experiment_name: str,\n",
    "    optuna_target_db: str = \"all_studies.db\",\n",
    "    local_folder: str = \"results\",\n",
    "    mlflow_artifact_folder: str = \"optuna_db\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Aggregate Optuna studies from MLflow experiment runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_name : str\n",
    "        MLflow experiment name\n",
    "    optuna_target_db : str\n",
    "        Target database filename for aggregated studies\n",
    "    local_folder : str\n",
    "        Local folder to download artifacts to\n",
    "    mlflow_artifact_folder : str\n",
    "        MLflow artifact folder name containing Optuna DBs\n",
    "    \"\"\"\n",
    "    Path(local_folder).mkdir(exist_ok=True)\n",
    "\n",
    "    # Download Optuna DB artifacts from all runs\n",
    "    download_artifacts_across_runs(\n",
    "        experiment_name, mlflow_artifact_folder, local_folder\n",
    "    )\n",
    "\n",
    "    # Combine all studies into single database\n",
    "    copy_all_studies_to_new_file(\n",
    "        f\"{local_folder}/{optuna_target_db}\",\n",
    "        f\"{local_folder}/{mlflow_artifact_folder}\",\n",
    "    )\n",
    "\n",
    "    print(f\"Aggregated studies saved to {local_folder}/{optuna_target_db}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate studies\n",
    "execute_study_agg_pipeline(\n",
    "    experiment_name=experiment_name,\n",
    "    optuna_target_db=\"all_fm_studies.db\",\n",
    "    local_folder=\"results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## View Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aggregated study\n",
    "study = optuna.load_study(\n",
    "    study_name=\"fm_gambling\",\n",
    "    storage=\"sqlite:///results/all_fm_studies.db\"\n",
    ")\n",
    "\n",
    "print(f\"Best RMSE: {-study.best_value:.4f}\")\n",
    "print(f\"Best params: {study.best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
