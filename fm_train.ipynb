{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# FM Training - Brand-Specific\n",
    "\n",
    "Factorization Machines training with MLflow and Optuna.\n",
    "Hyperparameter ranges vary per brand, data size is static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import optuna\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "region = session.boto_region_name\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "print(f'Region: {region}')\n",
    "print(f'Bucket: {bucket}')\n",
    "print(f'Account: {account_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": ["parameters"]
   },
   "outputs": [],
   "source": [
    "brand = 'betmax'\n",
    "max_trials = '10'\n",
    "early_stopping = '3'\n",
    "num_factors_min = '16'\n",
    "num_factors_max = '64'\n",
    "epochs_min = '10'\n",
    "epochs_max = '30'\n",
    "experiment_name = 'fm_betmax'\n",
    "project_name = 'fm-gambling-recommender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trials = int(max_trials)\n",
    "early_stopping = int(early_stopping)\n",
    "num_factors_min = int(num_factors_min)\n",
    "num_factors_max = int(num_factors_max)\n",
    "epochs_min = int(epochs_min)\n",
    "epochs_max = int(epochs_max)\n",
    "\n",
    "N_USERS = 500\n",
    "N_GAMES = 50\n",
    "\n",
    "print(f'Brand: {brand}')\n",
    "print(f'Data: {N_USERS} users, {N_GAMES} games')\n",
    "print(f'Trials: {max_trials}, Early stopping: {early_stopping}')\n",
    "print(f'Factors: {num_factors_min}-{num_factors_max}, Epochs: {epochs_min}-{epochs_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = 'true'\n",
    "os.environ['MLFLOW_TRACKING_URI'] = f'arn:aws:sagemaker:{region}:{account_id}:mlflow-tracking-server/{project_name}-dev-mlflow'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f'MLflow experiment: {experiment_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.simulate_gambling_data import generate_demo_data\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_demo_data(\n",
    "    n_users=N_USERS,\n",
    "    n_games=N_GAMES,\n",
    "    brand=brand,\n",
    ")\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "print(f'Train: {X_train.shape}, Test: {X_test.shape}')\n",
    "print(f'Features: {n_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.fm_sagemaker import write_to_s3\n",
    "\n",
    "prefix = f'fm-training/{brand}'\n",
    "train_path = write_to_s3(X_train, y_train, bucket, prefix, 'train/train.protobuf')\n",
    "test_path = write_to_s3(X_test, y_test, bucket, prefix, 'test/test.protobuf')\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "print(f'Data uploaded to s3://{bucket}/{prefix}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Optuna Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.fm_sagemaker import train_fm_model\n",
    "\n",
    "def objective(trial):\n",
    "    num_factors = trial.suggest_int('num_factors', num_factors_min, num_factors_max)\n",
    "    epochs = trial.suggest_int('epochs', epochs_min, epochs_max)\n",
    "    mini_batch_size = trial.suggest_categorical('mini_batch_size', [100, 200, 500])\n",
    "    \n",
    "    with mlflow.start_run(run_name=f'{brand}-trial-{trial.number}', nested=True):\n",
    "        mlflow.log_params({\n",
    "            'brand': brand,\n",
    "            'num_factors': num_factors,\n",
    "            'epochs': epochs,\n",
    "            'mini_batch_size': mini_batch_size,\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            fm = train_fm_model(\n",
    "                train_path=train_path,\n",
    "                test_path=test_path,\n",
    "                output_path=f'{output_path}/trial-{trial.number}',\n",
    "                role=role,\n",
    "                n_features=n_features,\n",
    "                num_factors=num_factors,\n",
    "                epochs=epochs,\n",
    "                mini_batch_size=mini_batch_size,\n",
    "            )\n",
    "            \n",
    "            job_name = fm.latest_training_job.name\n",
    "            sm = boto3.client('sagemaker')\n",
    "            metrics = sm.describe_training_job(TrainingJobName=job_name)\n",
    "            \n",
    "            test_rmse = 999.0\n",
    "            for m in metrics.get('FinalMetricDataList', []):\n",
    "                if m['MetricName'] == 'test:rmse':\n",
    "                    test_rmse = m['Value']\n",
    "                    break\n",
    "            \n",
    "            mlflow.log_metric('test_rmse', test_rmse)\n",
    "            print(f'Trial {trial.number}: RMSE={test_rmse:.4f}')\n",
    "            return test_rmse\n",
    "            \n",
    "        except Exception as e:\n",
    "            mlflow.log_param('error', str(e)[:200])\n",
    "            return 999.0\n",
    "\n",
    "\n",
    "def early_stop_callback(study, trial):\n",
    "    if len(study.trials) < early_stopping:\n",
    "        return\n",
    "    recent = [t.value for t in study.trials[-early_stopping:] if t.value and t.value < 999]\n",
    "    if len(recent) >= early_stopping and study.best_value:\n",
    "        if min(recent) >= study.best_value:\n",
    "            study.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f'{brand}-optuna-study'):\n",
    "    mlflow.log_params({\n",
    "        'brand': brand,\n",
    "        'n_users': N_USERS,\n",
    "        'n_games': N_GAMES,\n",
    "        'max_trials': max_trials,\n",
    "    })\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f'{brand}-fm-study',\n",
    "        direction='minimize',\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=max_trials,\n",
    "        callbacks=[early_stop_callback],\n",
    "    )\n",
    "    \n",
    "    mlflow.log_params({f'best_{k}': v for k, v in study.best_params.items()})\n",
    "    mlflow.log_metric('best_rmse', study.best_value)\n",
    "    \n",
    "    print(f'Best trial: {study.best_trial.number}')\n",
    "    print(f'Best RMSE: {study.best_value:.4f}')\n",
    "    print(f'Best params: {study.best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Optuna Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "fig = vis.plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_parallel_coordinate(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vis.plot_slice(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Brand: {brand}')\n",
    "print(f'Completed trials: {len(study.trials)}')\n",
    "print(f'Best RMSE: {study.best_value:.4f}')\n",
    "print(f'Best params: {study.best_params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
